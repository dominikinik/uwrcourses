{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGXgWugfJ0Vl"
      },
      "source": [
        "## Assignment 1\n",
        "\n",
        "**Submission deadlines:**\n",
        "- get at least **2** points by Tuesday, 27.02.2024\n",
        "- remaining points: last lab session before or on Tuesday, 05.03.2023\n",
        "\n",
        "**Points:** Aim to get 8 out of 14 possible points\n",
        "\n",
        "## Submission instructions\n",
        "The class is held on-site in lab rooms. Please prepare you notebook on your computer or anywhere in the cloud (try using DeepNote or Google Colab).\n",
        "\n",
        "Make sure you know all the questions and asnwers, and that the notebook contains results; before presentation do `Runtime -> Restart and run all`\n",
        "![Picture title](image-20220302-183151.png)\n",
        "\n",
        "We provide starter code, however you are not required to use it as long as you properly solve the tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S8iRaCPyO2a"
      },
      "source": [
        "# Task description\n",
        "\n",
        "## TLDR\n",
        "Implement and train a neural network using pure `torch`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTLhgnBdKDbe"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHcz7I2V-bVM"
      },
      "source": [
        "\n",
        "## Problem 1 [2p]\n",
        "Implement a two-layer network, manually set weights and biases to solve the XOR task.\n",
        "\n",
        "A two-layer neural network implementes a function $f: \\mathbb{R}^D \\rightarrow \\mathbb{R}^O$ where $D$ is the input dimensionality and $O$ is the output dinemsionality. The output goes through an intermediate representation (the hidden layer) with dimensionality $H$.\n",
        "\n",
        "The computations are as follows:\n",
        "$$\n",
        "\\begin{aligned}\n",
        "A_1 &= x W_1^T + b_1  & \\qquad\\text{Total input to neurons in the hidden layer (network's first layer)} \\\\\n",
        "O_1 &= \\sigma_1(A_1)  & \\qquad\\text{Output of the hidden layer} \\\\\n",
        "A_2 &= O_1 W_2^T + b_2 & \\qquad\\text{Total input to neurons in the output layer (network's second layer)}\\\\\n",
        "O_2 &= \\sigma_2(A_2)  & \\qquad\\text{Output of the network}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Where $W$ are weight matrices, $b$ are bias vectors, $\\sigma$ are non-linear activation functions (e.g. the logistic sigmoid applied element-wise, or softmax).\n",
        "\n",
        "For the 2D xor problem the network will:\n",
        "- have 2 inputs, 2 hidden neurons, one output\n",
        "- use the logistic sigmoid everywhere (that way we, when hand-designig the weights, we can assume that neurons' outputs are binary).\n",
        "\n",
        "Therrefore the shapes of the data flowing through the network will be:\n",
        "- input: $x\\in\\mathbb{}R^{2}$\n",
        "- hidden layer parameters: $W_1\\in\\mathbb{}R^{2\\times 2}$ and $b_1\\in\\mathbb{}R^{2}$\n",
        "- representations in the hidden layer: $A_1\\in\\mathbb{}R^{2}$ and $O_1\\in\\mathbb{}R^{2}$\n",
        "- output layer parameters: $W_2\\in\\mathbb{}R^{1\\times 2}$ and $b_2\\in\\mathbb{}R^{1}$\n",
        "- representations in the output layer: $A_2\\in\\mathbb{}R^{1}$ and $O_2\\in\\mathbb{}R^{1}$\n",
        "\n",
        "The network can be seen as a logistic regression model, prefixed by a nonlinear transformation of the data.\n",
        "\n",
        "The first tasks consists of:\n",
        "- implementing the network\n",
        "- selecting parametwrs ($W_1, b_1, W_2, b_2$) such that $f(x)\\approx XOR(x_1, x_2)$ where the approximation is die to the sigmoids - the output may be close to 0 or 1, but doesn't need to saturate at 0 or 1.\n",
        "\n",
        "NB: the convention on weight matrix shapes follows linear [layers in PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QSpZxuH-bLe"
      },
      "source": [
        "## Problem 2 [2p]\n",
        "1. Add a backward pass.\n",
        "2. Use a sensible random initialization for weights and biases.\n",
        "3. Numerically check the correctness of your gradient computation.\n",
        "\n",
        "There is nice article about taking derivative over vectors and vector chain rule: https://explained.ai/matrix-calculus/ if someone don't have experience with such calculus.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1Tn8j0m-bAy"
      },
      "source": [
        "## Problem 3 [2p]\n",
        "1. Implement gradient descent\n",
        "2. Train your network to solve 3D XOR\n",
        "3. Try several hidden layer sizes, for each size record the fracton of successful trainings. Then answer:\n",
        "    - What is the minimal hidden size required to solve 3D XOR (even with low reliability, when the training has to be repeated multiple times)\n",
        "    - What is the minimal hidden size required to reliably solve 3D XOR\n",
        "    - Which networks are easier to train - small or large ones? Why?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RP9Pvpmf-a2A"
      },
      "source": [
        "## Problem 4 [1p]\n",
        "Replace the first nonlinearity with the [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) activation function. Find a network architecture which reliably learns the 3D XOR problem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGgtpe-w-asB"
      },
      "source": [
        "## Problem 5 [1p]\n",
        "Add a second hidden layer to your network, implement the forward and backward pass, then demonstrate training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pe-pcFeO-aiE"
      },
      "source": [
        "## Problem 6 [2p]\n",
        "Implement a way to have a _variable number_ of hidden layers. Check how deep sigmoid or ReLU networks you  can train. For simplicity you can assume that all hidden layers have the same number of neurons, and use the same activation function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M20iTZ6IL_Pb"
      },
      "source": [
        "## Problem 7 [2p]\n",
        "Consider the following function $\\boldsymbol f: \\mathbb R^n \\to \\mathbb R^k$:\n",
        "$$ \\hat{\\boldsymbol y} := \\boldsymbol f(\\boldsymbol x\\,;\\,\\,W, \\boldsymbol b) = \\mathrm{softmax}(\\boldsymbol z) \\qquad (\\boldsymbol x \\in \\mathbb R^{n}, \\, W \\in\\mathbb R^{k\\times n}, \\, \\boldsymbol b \\in \\mathbb R^k), $$\n",
        "where $$ \\boldsymbol z = W  \\boldsymbol x + \\boldsymbol b $$\n",
        "and $W, \\boldsymbol b$ are the parameters.\n",
        "\n",
        "Analytically derive the form of the following gradients:\n",
        "\n",
        "$$ \\frac{\\partial \\hat{\\boldsymbol y}}{\\boldsymbol z} = \\ldots $$\n",
        "\n",
        "$$ \\frac{\\partial \\hat{\\boldsymbol y}}{W_{ij}} = \\ldots $$\n",
        "\n",
        "$$ \\frac{\\partial \\hat{\\boldsymbol y}}{\\boldsymbol b} = \\ldots $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXJaoHSH0DZO"
      },
      "source": [
        "# Solutions and starter code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiTEWD2oqW0Y"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import matmul"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqtfJKR40J3x"
      },
      "source": [
        "XOR dataset creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "85w0A-oML_Pc",
        "outputId": "fa6a532e-dd64-4f12-d4d2-cb9a9ea9e473"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.05, 1.05, -0.05, 1.05)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Let's define a XOR dataset\n",
        "\n",
        "# X will be matrix of N 2-dimensional inputs\n",
        "X = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1],], dtype=torch.float64)\n",
        "# Y is a matrix of N numners - answers\n",
        "Y = torch.tensor([[0], [1], [1], [0],], dtype=torch.float64)\n",
        "\n",
        "plt.scatter(\n",
        "    X[:, 0], X[:, 1], c=Y[:, 0],\n",
        ")\n",
        "plt.xlabel(\"X[0]\")\n",
        "plt.ylabel(\"X[1]\")\n",
        "plt.axis(\"square\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rb3azMn929_I"
      },
      "source": [
        "## Problem 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZCM_hdELE04"
      },
      "source": [
        "The code below contains a mock-up of a two-layer neural network. Fill in the code and manually set weights to solve the XOR problem.\n",
        "\n",
        "Please note: the shapes are set to be compatible with PyTorch's conventions:\n",
        "* a batch containing $N$ $D$-dimensional examples has shape $N\\times D$ (each example is a row!)\n",
        "* a weight matrix in a linear layer with $I$ inputs and $O$ outputs has shape $O \\times I$\n",
        "* a bias vector is a 1D vector. Please note that [broadcasting rules](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html) allow us to think about it as a $1 \\times D$ matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrrRuk6zLiF0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def sigmoid(x,deriv=False):\n",
        "  if not deriv:\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "  return 1 / (1 + np.exp(-x)) * (1 - 1 / (1 + np.exp(-x)))\n",
        "def relu(x,deriv=False):\n",
        "  if not deriv:\n",
        "    return np.maximum(0, x)\n",
        "  return np.where(x > 0, 1, 0)\n",
        "\n",
        "class SmallNet:\n",
        "    def __init__(self, in_features, num_hidden, dtype=np.float64):\n",
        "        self.W1 = np.zeros((num_hidden, in_features), dtype=dtype)\n",
        "        self.b1 = np.zeros((num_hidden,), dtype=dtype)\n",
        "        self.W2 = np.zeros((1, num_hidden), dtype=dtype)\n",
        "        self.b2 = np.zeros((1,), dtype=dtype)\n",
        "        self.init_params(in_features, num_hidden)\n",
        "\n",
        "\n",
        "    def init_params(self, in_features, num_hidden):\n",
        "        # TODO for Problem 2:\n",
        "        # set all parameters to small random values, e.g. from N(0, 0.5)\n",
        "        self.W1 = np.random.normal(scale = 0.5, size = (num_hidden, in_features))\n",
        "        self.b1 = np.random.normal(scale = 0.5, size = (num_hidden,))\n",
        "        self.W2 = np.random.normal(scale = 0.5, size = (1, num_hidden))\n",
        "        self.b2 = np.random.normal(scale = 0.5, size = (1,))\n",
        "\n",
        "    def forward(self, X, Y=None, do_backward=False, actfunc = sigmoid):\n",
        "        # TODO Problem 1: Fill in details of forward propagation\n",
        "        # Input to neurons in 1st layer\n",
        "        A1 = X @ self.W1.T + self.b1\n",
        "       # print(\"a1\",A1)\n",
        "        # Outputs after the sigmoid non-linearity\n",
        "        O1 = actfunc(A1)\n",
        "        #print(\"o1\",O1)\n",
        "        # Inputs to neuron in the second layer\n",
        "        A2 = O1 @ self.W2.T + self.b2\n",
        "        #print(\"a2\",A2)\n",
        "        # Outputs after the sigmoid non-linearity\n",
        "        O2 = sigmoid(A2)\n",
        "       # print(\"o2\",O2)\n",
        "        # When Y is none, simply return the predictions. Else compute the loss\n",
        "        if Y is not None:\n",
        "            loss = -(Y * np.log(O2) + (1 - Y) * np.log(1.0 - O2))\n",
        "            loss = loss.sum() / X.shape[0]\n",
        "        else:\n",
        "            loss = np.nan\n",
        "\n",
        "        if do_backward:\n",
        "\n",
        "            # TODO in Problem 2:\n",
        "            # fill in the gradient computation\n",
        "            # Please note, that there is a correspondance between\n",
        "            # the forward and backward pass: with backward computations happening\n",
        "            # in reverse order.\n",
        "            # We save the gradients with respect to the parameters as fields of self.\n",
        "            # It is not very elegant, but simplifies training code later on.\n",
        "\n",
        "            # A2_grad is the gradient of loss with respect to A2\n",
        "            # Hint: there is a concise formula for the gradient\n",
        "            # of logistic sigmoid and cross-entropy loss\n",
        "\n",
        "            A2_grad = (O2 - Y) # 1 1 elementy wykladu\n",
        "            # print(\"A2\",A2_grad)\n",
        "            self.b2_grad = A2_grad.sum(0)/ A2_grad.shape[0]#  1 3 2\n",
        "            #print(\"B2\",self.b2_grad )#\n",
        "            self.W2_grad = A2_grad.T @ O1 / A2_grad.shape[0]# 1 3\n",
        "            O1_grad = A2_grad @ self.W2  # 2 1\n",
        "\n",
        "            A1_grad = O1_grad * actfunc(A1,True) # 2 2\n",
        "\n",
        "            self.b1_grad = A1_grad.sum(0) / A2_grad.shape[0]# 2 3 2\n",
        "            self.W1_grad = A1_grad.T @ X / A1_grad.shape[0]# 2 3\n",
        "\n",
        "        return O2, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJswvBk0oiIY",
        "outputId": "e4f7b710-eaea-4f3b-e7f5-c9ea7c1f5179"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XORnet(tensor([0., 0.], dtype=torch.float64)) = tensor([0.5252], dtype=torch.float64)\n",
            "XORnet(tensor([0., 1.], dtype=torch.float64)) = tensor([0.5220], dtype=torch.float64)\n",
            "XORnet(tensor([1., 0.], dtype=torch.float64)) = tensor([0.5277], dtype=torch.float64)\n",
            "XORnet(tensor([1., 1.], dtype=torch.float64)) = tensor([0.5248], dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "# TODO Problem 1:\n",
        "# Set by hand the weight values to solve the XOR problem\n",
        "import numpy as np\n",
        "net = SmallNet(2, 2, dtype=np.float64)\n",
        "\n",
        "\n",
        "\n",
        "# Hint: since we use the logistic sigmoid activation, the weights may need to\n",
        "# be fairly large\n",
        "\n",
        "predictions, loss = net.forward(X, Y, do_backward=True)\n",
        "for x, p in zip(X, predictions):\n",
        "    print(f\"XORnet({x}) = {p}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmxCi5Vl6_xB"
      },
      "source": [
        "## Problem 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSM5hgJ1mrhY"
      },
      "outputs": [],
      "source": [
        "def check_grad(net, param_name, X, Y, eps=1e-5):\n",
        "    \"\"\"A gradient checking routine\"\"\"\n",
        "\n",
        "    param = getattr(net, param_name)\n",
        "    param_flat_accessor = param.reshape(-1)\n",
        "\n",
        "    grad = np.empty_like(param)\n",
        "    grad_flat_accessor = grad.reshape(-1)\n",
        "\n",
        "    net.forward(X, Y, do_backward=True)\n",
        "    orig_grad = getattr(net, param_name + \"_grad\")\n",
        "    assert param.shape == orig_grad.shape\n",
        "\n",
        "    for i in range(param_flat_accessor.shape[0]):\n",
        "        orig_val = param_flat_accessor[i]\n",
        "        param_flat_accessor[i] = orig_val + eps\n",
        "        _, loss_positive = net.forward(X, Y)\n",
        "        param_flat_accessor[i] = orig_val - eps\n",
        "        _, loss_negative = net.forward(X, Y)\n",
        "        param_flat_accessor[i] = orig_val\n",
        "        grad_flat_accessor[i] = (loss_positive - loss_negative) / (2 * eps)\n",
        "    assert np.allclose(grad, orig_grad)\n",
        "    print(\"grad\",grad)\n",
        "    print(\"orig\",orig_grad)\n",
        "    print()\n",
        "    return grad, orig_grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTZu0jFEvgXF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39290446-fead-464f-fcd1-33dca73113f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grad [[0.06171831 0.06060681]\n",
            " [0.03922224 0.03998693]]\n",
            "orig tensor([[0.0617, 0.0606],\n",
            "        [0.0392, 0.0400]], dtype=torch.float64)\n",
            "\n",
            "grad [0.12238442 0.07900317]\n",
            "orig tensor([0.1224, 0.0790], dtype=torch.float64)\n",
            "\n",
            "grad [[-0.17997085 -0.14089559]]\n",
            "orig tensor([[-0.1800, -0.1409]], dtype=torch.float64)\n",
            "\n",
            "grad [-0.37095668]\n",
            "orig tensor([-0.3710], dtype=torch.float64)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Hint: use float64 for checking the correctness of the gradient\n",
        "net = SmallNet(2, 2, dtype=np.float64)\n",
        "\n",
        "\n",
        "for param_name in [\"W1\", \"b1\", \"W2\", \"b2\"]:\n",
        "    check_grad(net, param_name, X, Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mUOs3cVvjM2"
      },
      "source": [
        "## Problem 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nn2AAoZo0vjU"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(net, alpha, X, Y, actfunc=sigmoid):\n",
        "    for i in range(100000):\n",
        "        net_result, loss = net.forward(X, Y, do_backward=True, actfunc=actfunc)\n",
        "        if (i % 5000) == 0:\n",
        "            print(f\"{i} i \\tloss={loss}\")\n",
        "        for param_name in [\"W1\", \"b1\", \"W2\", \"b2\"]:\n",
        "            param = getattr(net, param_name)\n",
        "            grad_param = getattr(net, param_name + \"_grad\")\n",
        "            # Convert tensor to numpy array\n",
        "            param = np.array(param)\n",
        "            grad_param = np.array(grad_param)\n",
        "            param[:] = param - alpha * grad_param\n",
        "            # Set updated numpy array back to network parameter\n",
        "            setattr(net, param_name, param)\n",
        "\n",
        "    return net_result, loss, param"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "TwpEjpkU1JvK",
        "outputId": "2734a49e-a18c-4598-fcde-0e790e472735"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 i \tloss=0.7010466192201348\n",
            "5000 i \tloss=0.036296782716483136\n",
            "10000 i \tloss=0.010474812393589154\n",
            "15000 i \tloss=0.005882141551051386\n",
            "20000 i \tloss=0.004034421331434398\n",
            "25000 i \tloss=0.0030489445432268078\n",
            "30000 i \tloss=0.002440146786741662\n",
            "35000 i \tloss=0.00202829667607213\n",
            "40000 i \tloss=0.0017319052878136555\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-df9399748bd5>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSmallNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-eb0d1c130b1c>\u001b[0m in \u001b[0;36mgradient_descent\u001b[0;34m(net, alpha, X, Y, actfunc)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mnet_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_backward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mactfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{i} i \\tloss={loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-b46264d3f6c7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X, Y, do_backward, actfunc)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m#print(\"a2\",A2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# Outputs after the sigmoid non-linearity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mO2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m        \u001b[0;31m# print(\"o2\",O2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# When Y is none, simply return the predictions. Else compute the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-b46264d3f6c7>\u001b[0m in \u001b[0;36msigmoid\u001b[0;34m(x, deriv)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mderiv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mderiv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mderiv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0m__array_priority__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m  \u001b[0;31m# prefer Tensor ops over numpy ones\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_unary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "net = SmallNet(2, 10, dtype=np.float64)\n",
        "_ = gradient_descent(net, 0.1, X, Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0ZMyHqz8xrC",
        "outputId": "78cb1384-85d4-43cc-e0e9-42a132712822"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "size = 2\n",
            "0 i \tloss=0.6970554263675982\n",
            "5000 i \tloss=0.6931633686318306\n",
            "10000 i \tloss=0.6928541264534673\n",
            "15000 i \tloss=0.688364781181837\n",
            "20000 i \tloss=0.4219879176021763\n",
            "25000 i \tloss=0.3652783476591328\n",
            "30000 i \tloss=0.35663031474094864\n",
            "35000 i \tloss=0.35335851709086474\n",
            "40000 i \tloss=0.35166363451414107\n",
            "45000 i \tloss=0.3506335852266336\n",
            "50000 i \tloss=0.34994374437170916\n",
            "55000 i \tloss=0.34945053138169757\n",
            "60000 i \tloss=0.34908091305172806\n",
            "65000 i \tloss=0.34879391093113693\n",
            "70000 i \tloss=0.348564793693128\n",
            "75000 i \tloss=0.34837776965292444\n",
            "80000 i \tloss=0.34822229158034285\n",
            "85000 i \tloss=0.34809105151786524\n",
            "90000 i \tloss=0.3479788287734802\n",
            "95000 i \tloss=0.3478817961145047\n",
            "\n",
            "LOSS: 0.3477971008768018\n",
            "RESULT:\n",
            "XOR([1. 1. 1.]) = [0.99883201]  Y: [1.]\n",
            "XOR([1. 1. 0.]) = [0.50092911]  Y: [0.]\n",
            "XOR([1. 0. 1.]) = [0.50083162]  Y: [0.]\n",
            "XOR([1. 0. 0.]) = [0.99889373]  Y: [1.]\n",
            "XOR([0. 1. 1.]) = [0.00115184]  Y: [0.]\n",
            "XOR([0. 1. 0.]) = [0.49963282]  Y: [1.]\n",
            "XOR([0. 0. 1.]) = [0.49955936]  Y: [1.]\n",
            "XOR([0. 0. 0.]) = [0.00121839]  Y: [0.]\n",
            "size = 3\n",
            "0 i \tloss=0.8602604462090702\n",
            "5000 i \tloss=0.6920186343906587\n",
            "10000 i \tloss=0.41148873490794746\n",
            "15000 i \tloss=0.031253357149520454\n",
            "20000 i \tloss=0.013287099550748535\n",
            "25000 i \tloss=0.008500808077694676\n",
            "30000 i \tloss=0.006273595464561665\n",
            "35000 i \tloss=0.004981093718784831\n",
            "40000 i \tloss=0.004135034239398194\n",
            "45000 i \tloss=0.0035373241773104316\n",
            "50000 i \tloss=0.003092184379953426\n",
            "55000 i \tloss=0.002747577222309585\n",
            "60000 i \tloss=0.002472764777090173\n",
            "65000 i \tloss=0.0022484085088184938\n",
            "70000 i \tloss=0.002061726060176457\n",
            "75000 i \tloss=0.0019039261225837244\n",
            "80000 i \tloss=0.001768761366253751\n",
            "85000 i \tloss=0.0016516690133937522\n",
            "90000 i \tloss=0.0015492381044792734\n",
            "95000 i \tloss=0.001458867105931291\n",
            "\n",
            "LOSS: 0.001378552094464967\n",
            "RESULT:\n",
            "XOR([1. 1. 1.]) = [0.99989387]  Y: [1.]\n",
            "XOR([1. 1. 0.]) = [0.00185108]  Y: [0.]\n",
            "XOR([1. 0. 1.]) = [0.00158294]  Y: [0.]\n",
            "XOR([1. 0. 0.]) = [0.99821773]  Y: [1.]\n",
            "XOR([0. 1. 1.]) = [0.0015247]  Y: [0.]\n",
            "XOR([0. 1. 0.]) = [0.9981784]  Y: [1.]\n",
            "XOR([0. 0. 1.]) = [0.99779351]  Y: [1.]\n",
            "XOR([0. 0. 0.]) = [0.00014337]  Y: [0.]\n",
            "size = 5\n",
            "0 i \tloss=0.7242940427488072\n",
            "5000 i \tloss=0.6925424833023359\n",
            "10000 i \tloss=0.6475604512311417\n",
            "15000 i \tloss=0.034171835011373566\n",
            "20000 i \tloss=0.011294401501476483\n",
            "25000 i \tloss=0.006252821096718297\n",
            "30000 i \tloss=0.004201579499985199\n",
            "35000 i \tloss=0.003120978375667474\n",
            "40000 i \tloss=0.002463706923023175\n",
            "45000 i \tloss=0.002025562512845705\n",
            "50000 i \tloss=0.0017143540970881738\n",
            "55000 i \tloss=0.0014827810709044937\n",
            "60000 i \tloss=0.001304232040900962\n",
            "65000 i \tloss=0.0011626546984553731\n",
            "70000 i \tloss=0.0010478225812272876\n",
            "75000 i \tloss=0.0009529278377462614\n",
            "80000 i \tloss=0.000873271466115914\n",
            "85000 i \tloss=0.0008055103192031384\n",
            "90000 i \tloss=0.0007472038842193505\n",
            "95000 i \tloss=0.000696530702463897\n",
            "\n",
            "LOSS: 0.0006521132683853441\n",
            "RESULT:\n",
            "XOR([1. 1. 1.]) = [0.99880832]  Y: [1.]\n",
            "XOR([1. 1. 0.]) = [0.00053302]  Y: [0.]\n",
            "XOR([1. 0. 1.]) = [0.00052698]  Y: [0.]\n",
            "XOR([1. 0. 0.]) = [0.99954023]  Y: [1.]\n",
            "XOR([0. 1. 1.]) = [0.00027566]  Y: [0.]\n",
            "XOR([0. 1. 0.]) = [0.99934321]  Y: [1.]\n",
            "XOR([0. 0. 1.]) = [0.99922396]  Y: [1.]\n",
            "XOR([0. 0. 0.]) = [0.00079501]  Y: [0.]\n",
            "size = 10\n",
            "0 i \tloss=0.8377653419868321\n",
            "5000 i \tloss=0.6914634561329358\n",
            "10000 i \tloss=0.20026368868361602\n",
            "15000 i \tloss=0.013554355272498967\n",
            "20000 i \tloss=0.006062999287114025\n",
            "25000 i \tloss=0.003805986881341222\n",
            "30000 i \tloss=0.002745777280413069\n",
            "35000 i \tloss=0.002136442465581895\n",
            "40000 i \tloss=0.0017430252704782332\n",
            "45000 i \tloss=0.0014689789054343648\n",
            "50000 i \tloss=0.0012675937257062406\n",
            "55000 i \tloss=0.0011136049143726004\n",
            "60000 i \tloss=0.000992188437388773\n",
            "65000 i \tloss=0.0008940888036538402\n",
            "70000 i \tloss=0.0008132367446491396\n",
            "75000 i \tloss=0.0007454909888597238\n",
            "80000 i \tloss=0.0006879313766867157\n",
            "85000 i \tloss=0.0006384410748732532\n",
            "90000 i \tloss=0.00059544896595252\n",
            "95000 i \tloss=0.0005577649831237997\n",
            "\n",
            "LOSS: 0.0005244778178468711\n",
            "RESULT:\n",
            "XOR([1. 1. 1.]) = [0.99950973]  Y: [1.]\n",
            "XOR([1. 1. 0.]) = [0.00051056]  Y: [0.]\n",
            "XOR([1. 0. 1.]) = [0.00049274]  Y: [0.]\n",
            "XOR([1. 0. 0.]) = [0.99950928]  Y: [1.]\n",
            "XOR([0. 1. 1.]) = [0.00047346]  Y: [0.]\n",
            "XOR([0. 1. 0.]) = [0.99936963]  Y: [1.]\n",
            "XOR([0. 0. 1.]) = [0.9994653]  Y: [1.]\n",
            "XOR([0. 0. 0.]) = [0.00057189]  Y: [0.]\n",
            "size = 20\n",
            "0 i \tloss=0.7821632489134187\n",
            "5000 i \tloss=0.59061856640511\n",
            "10000 i \tloss=0.02363729594848083\n",
            "15000 i \tloss=0.008347862865239548\n",
            "20000 i \tloss=0.0047608638718472564\n",
            "25000 i \tloss=0.003251102599348481\n",
            "30000 i \tloss=0.0024383376111999653\n",
            "35000 i \tloss=0.0019365488966676028\n",
            "40000 i \tloss=0.0015984243717920331\n",
            "45000 i \tloss=0.0013563119658404179\n",
            "50000 i \tloss=0.0011750333760851394\n",
            "55000 i \tloss=0.0010345826899732118\n",
            "60000 i \tloss=0.0009227847360628034\n",
            "65000 i \tloss=0.0008318254995423612\n",
            "70000 i \tloss=0.0007564704877184398\n",
            "75000 i \tloss=0.0006930873173186291\n",
            "80000 i \tloss=0.0006390797528869805\n",
            "85000 i \tloss=0.0005925449190823588\n",
            "90000 i \tloss=0.0005520576298752564\n",
            "95000 i \tloss=0.0005165301932273064\n",
            "\n",
            "LOSS: 0.00048512457830746596\n",
            "RESULT:\n",
            "XOR([1. 1. 1.]) = [0.99957067]  Y: [1.]\n",
            "XOR([1. 1. 0.]) = [0.00043289]  Y: [0.]\n",
            "XOR([1. 0. 1.]) = [0.00044147]  Y: [0.]\n",
            "XOR([1. 0. 0.]) = [0.99950519]  Y: [1.]\n",
            "XOR([0. 1. 1.]) = [0.00048437]  Y: [0.]\n",
            "XOR([0. 1. 0.]) = [0.99946117]  Y: [1.]\n",
            "XOR([0. 0. 1.]) = [0.99949697]  Y: [1.]\n",
            "XOR([0. 0. 0.]) = [0.0005553]  Y: [0.]\n"
          ]
        }
      ],
      "source": [
        "# TODO:\n",
        "# Generate data for a 3D XOR task\n",
        "# Then estimate the success rate of training the network with diferent\n",
        "# hidden sizes.\n",
        "import itertools\n",
        "values = [1, 0]\n",
        "X3 = np.zeros(shape = (8, 3), dtype = np.float64)\n",
        "Y3 = np.zeros(shape = (8, 1), dtype = np.float64)\n",
        "for index, input in enumerate(itertools.product(values, repeat = 3)):\n",
        "    X3[index, :] = input\n",
        "    Y3[index, 0] = input[0] ^ input[1] ^ input[2]\n",
        "# print(X3)\n",
        "# print(Y3)\n",
        "for hidden_dim in [2, 3, 5, 10, 20]:\n",
        "    # TODO: run a few trainings and record the fraction of successful ones\n",
        "    print(f\"size = {hidden_dim}\")\n",
        "\n",
        "    net = SmallNet(3, hidden_dim, dtype=np.float64)\n",
        "    res, loss, _ = gradient_descent(net,alpha = 0.1,X = X3,Y = Y3,actfunc = sigmoid)\n",
        "    print(\"\\nLOSS:\", loss)\n",
        "    print(\"RESULT:\")\n",
        "    for input, output, y in zip(X3, res, Y3):\n",
        "        print(f\"XOR({input}) = {output}  Y: {y}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuaLEoV-9DLG"
      },
      "source": [
        "## Problem 4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3lk9_TM-MvK"
      },
      "outputs": [],
      "source": [
        "for hidden_dim in [2, 3, 5, 10, 20,40,80]:\n",
        "    print(f\"size= {hidden_dim}\")\n",
        "\n",
        "    net = SmallNet(3, hidden_dim, dtype=np.float64)\n",
        "    res, loss, params = gradient_descent(net,alpha = 0.1,X = X3,Y = Y3,actfunc = relu)\n",
        "    print(\"\\nLOSS:\", loss)\n",
        "    print(\"RESULT:\")\n",
        "    for input, output, y in zip(X3, res, Y3):\n",
        "        print(f\"XOR({input}) = {output}  Y: {y}\")\n",
        "\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Hr_iAKX-ND1"
      },
      "source": [
        "## Problem 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnz6CndQ-NRI"
      },
      "outputs": [],
      "source": [
        "class TwoLayerNet:\n",
        "    def __init__(self, in_features, num_first_hidden, num_second_hidden, dtype=np.float32):\n",
        "\n",
        "        self.W1 = np.zeros((num_first_hidden, in_features), dtype=dtype)\n",
        "        self.b1 = np.zeros((num_first_hidden,), dtype=dtype)\n",
        "        self.W2 = np.zeros((num_second_hidden, num_first_hidden), dtype=dtype)\n",
        "        self.b2 = np.zeros((num_second_hidden,), dtype=dtype)\n",
        "        self.W3 = np.zeros((1, num_second_hidden), dtype=dtype)\n",
        "        self.b3 = np.zeros((1,), dtype=dtype)\n",
        "        self.init_params(in_features, num_first_hidden, num_second_hidden)\n",
        "\n",
        "    def init_params(self, in_features, num_first_hidden, num_second_hidden):\n",
        "        # TODO for Problem 2:\n",
        "        # set all parameters to small random values, e.g. from N(0, 0.5)\n",
        "        self.W1 = np.random.normal(scale = 0.5, size = (num_first_hidden, in_features))\n",
        "        self.b1 = np.random.normal(scale = 0.5, size = (num_first_hidden,))\n",
        "        self.W2 = np.random.normal(scale = 0.5, size = (num_second_hidden, num_first_hidden))\n",
        "        self.b2 = np.random.normal(scale = 0.5, size = (num_second_hidden,))\n",
        "        self.W3 = np.random.normal(scale = 0.5, size = (1, num_second_hidden))\n",
        "        self.b3 = np.random.normal(scale = 0.5, size = (1,))\n",
        "\n",
        "    def forward(self, X, Y=None, do_backward=False):\n",
        "        # TODO Problem 1: Fill in details of forward propagation\n",
        "        # Input to neurons in 1st layer\n",
        "        A1 = X @ self.W1.T + self.b1\n",
        "\n",
        "        O1 = sigmoid(A1)\n",
        "        A2 = O1 @ self.W2.T + self.b2\n",
        "        O2 = sigmoid(A2)\n",
        "        A3 = O2 @ self.W3.T + self.b3\n",
        "        O3 = sigmoid(A3)\n",
        "        if Y is not None:\n",
        "            loss = -(Y * np.log(O3) + (1 - Y) * np.log(1.0 - O3))\n",
        "            loss = loss.sum() / X.shape[0]\n",
        "        else:\n",
        "            loss = np.nan\n",
        "\n",
        "        if do_backward:\n",
        "            # TODO in Problem 2:\n",
        "            # fill in the gradient computation\n",
        "            # Please note, that there is a correspondance between\n",
        "            # the forward and backward pass: with backward computations happening\n",
        "            # in reverse order.\n",
        "            # We save the gradients with respect to the parameters as fields of self.\n",
        "            # It is not very elegant, but simplifies training code later on.\n",
        "\n",
        "            # A2_grad is the gradient of loss with respect to A2\n",
        "            # Hint: there is a concise formula for the gradient\n",
        "            # of logistic sigmoid and cross-entropy loss\n",
        "\n",
        "            A3_grad = O3 - Y\n",
        "            O2_grad = A3_grad @ self.W3\n",
        "            A2_grad = O2_grad * sigmoid(A2,True)\n",
        "            O1_grad = A2_grad @ self.W2\n",
        "            A1_grad = O1_grad *  sigmoid(A1,True)\n",
        "\n",
        "            self.b3_grad = A3_grad.sum(0) / A3_grad.shape[0]\n",
        "            self.W3_grad = A3_grad.T @ O2 / A3_grad.shape[0]\n",
        "            self.b2_grad = A2_grad.sum(0) / A2_grad.shape[0]\n",
        "            self.W2_grad = A2_grad.T @ O1 / A2_grad.shape[0]\n",
        "            self.b1_grad = A1_grad.sum(0) / A2_grad.shape[0]\n",
        "            self.W1_grad = A1_grad.T @ X / A1_grad.shape[0]\n",
        "\n",
        "        return O3, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHTyDwiZaz_G"
      },
      "outputs": [],
      "source": [
        "def gradient_descent_twolayered(net, alpha, X, Y):\n",
        "    for i in range(100000):\n",
        "        net_result, loss = net.forward(X, Y, do_backward=True)\n",
        "        if (i % 5000) == 0 :\n",
        "            print(f\"{i} i \\tloss={loss}\")\n",
        "        for param_name in [\"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"]:\n",
        "            param = getattr(net, param_name)\n",
        "            # Hint: use the construct `param[:]` to change the contents of the array!\n",
        "            # Doing instead `param = new_val` simply changes to what the variable\n",
        "            # param points to, without affecting the network!\n",
        "            # alternatively, you could do setattr(net, param_name, new_value)\n",
        "            param[:] = param - alpha * getattr(net, param_name + \"_grad\")\n",
        "    return net_result, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "8OnORPx-bCdD",
        "outputId": "2a65f98e-0ece-4f29-9801-29ad63d37c08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "size = (3, 1)\n",
            "0 i \tloss=0.7036967431861787\n",
            "5000 i \tloss=0.6931415722693041\n",
            "10000 i \tloss=0.6931381265961987\n",
            "15000 i \tloss=0.6931331434545005\n",
            "20000 i \tloss=0.6931252642786424\n",
            "25000 i \tloss=0.6931114881533863\n",
            "30000 i \tloss=0.693083794483949\n",
            "35000 i \tloss=0.693013770049397\n",
            "40000 i \tloss=0.6927331444224272\n",
            "45000 i \tloss=0.6882092395615288\n",
            "50000 i \tloss=0.5593454830234483\n",
            "55000 i \tloss=0.014097608839629884\n",
            "60000 i \tloss=0.00579398612076062\n",
            "65000 i \tloss=0.0036423238924769077\n",
            "70000 i \tloss=0.0026567412803282504\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-2e46a75fe6da>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"size = ({first_hidden_dim}, {second_hidden_dim})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTwoLayerNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_hidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond_hidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_descent_twolayered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nLOSS:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RESULT:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-7244d4fc4f67>\u001b[0m in \u001b[0;36mgradient_descent_twolayered\u001b[0;34m(net, alpha, X, Y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgradient_descent_twolayered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mnet_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_backward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{i} i \\tloss={loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-ee830aed05af>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X, Y, do_backward)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mO3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mY\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mO3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mO3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "dims = [[3, 1], [3, 2],[3,3],[2,2]]\n",
        "for first_hidden_dim, second_hidden_dim in dims:\n",
        "    print(f\"size = ({first_hidden_dim}, {second_hidden_dim})\")\n",
        "    net = TwoLayerNet(3, first_hidden_dim, second_hidden_dim, dtype=np.float64)\n",
        "    res, loss = gradient_descent_twolayered(net,alpha = 0.1,X = X3,Y = Y3)\n",
        "    print(\"\\nLOSS:\", loss)\n",
        "    print(\"RESULT:\")\n",
        "    for input, output, y in zip(X3, res, Y3):\n",
        "        print(f\"XOR({input}) = {output}  Y: {y}\")\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PcNxrCt-NcN"
      },
      "source": [
        "## Problem 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Brepirl-Nln"
      },
      "outputs": [],
      "source": [
        "\n",
        "class NLayerNet:\n",
        "    def genhiddenlayer(self , dtype=np.float32):\n",
        "          for i in range(2,self.num_of_hidden_lay):\n",
        "\n",
        "            wi= np.zeros((self.num_neurons_hidden_neur, self.num_neurons_hidden_neur), dtype=dtype)\n",
        "            bi= np.zeros((self.num_neurons_hidden_neur,), dtype=dtype)\n",
        "            self.hidden[i]=wi,bi\n",
        "\n",
        "          wend= np.zeros((1, self.num_neurons_hidden_neur), dtype=dtype)\n",
        "          bend= np.zeros((1,), dtype=dtype)\n",
        "\n",
        "          self.hidden[self.num_of_hidden_lay]=wend,bend\n",
        "\n",
        "\n",
        "    def __init__(self, in_features, num_of_hidden_lay, num_neurons_hidden_neur, dtype=np.float32):\n",
        "\n",
        "           self.verbote=False\n",
        "           self.deklarowanietablic = num_of_hidden_lay + 2\n",
        "           self.num_of_hidden_lay=num_of_hidden_lay+1\n",
        "\n",
        "           self.num_neurons_hidden_neur=num_neurons_hidden_neur\n",
        "           self.in_features=in_features\n",
        "\n",
        "           self.hidden = np.empty((self.deklarowanietablic, 2), dtype=object)\n",
        "           self.hidden_grad =np.empty((self.deklarowanietablic, 2), dtype=object)\n",
        "\n",
        "           W1 = np.zeros((num_neurons_hidden_neur, in_features), dtype=dtype)\n",
        "           b1 = np.zeros((num_neurons_hidden_neur,), dtype=dtype)\n",
        "           self.hidden[1]=W1,b1\n",
        "\n",
        "           self.genhiddenlayer()\n",
        "           self.init_params()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def init_params(self):\n",
        "        self.hidden[1][0]=np.random.normal(scale = 0.5, size = (self.num_neurons_hidden_neur, self.in_features))\n",
        "        self.hidden[1][1]= np.random.normal(scale = 0.5, size = (self.num_neurons_hidden_neur,))\n",
        "        for i in range(2,self.num_of_hidden_lay):\n",
        "          self.hidden[i][0]=np.random.normal(scale = 0.5, size = (self.num_neurons_hidden_neur, self.num_neurons_hidden_neur))\n",
        "          self.hidden[i][1]= np.random.normal(scale = 0.5, size = (self.num_neurons_hidden_neur,))\n",
        "\n",
        "        self.hidden[self.num_of_hidden_lay][0]= np.random.normal(scale = 0.5, size = (1, self.num_neurons_hidden_neur))\n",
        "        self.hidden[self.num_of_hidden_lay][1]= np.random.normal(scale = 0.5, size = (1,))\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, X, Y=None, do_backward=False):\n",
        "        A = np.zeros(self.deklarowanietablic, dtype=object)\n",
        "        O = np.zeros(self.deklarowanietablic, dtype=object)\n",
        "\n",
        "        A[1] = X @ self.hidden[1][0].T + self.hidden[1][1]\n",
        "        O[1]=sigmoid(A[1])\n",
        "\n",
        "\n",
        "        if self.verbote:\n",
        "            print( \"A\",A[0].shape[0],A[0].shape[1])\n",
        "            print( \"O\",O[0].shape[0],O[0].shape[1])\n",
        "\n",
        "        for i in range(2,self.deklarowanietablic):\n",
        "\n",
        "          A[i] = O[i-1] @ self.hidden[i][0].T+ self.hidden[i][1]\n",
        "          O[i]=sigmoid(A[i])\n",
        "\n",
        "\n",
        "          if self.verbote:\n",
        "            print( \"A\",A[i].shape[0],A[i].shape[1])\n",
        "            print( \"O\",O[i].shape[0],O[i].shape[1])\n",
        "\n",
        "\n",
        "\n",
        "        if self.verbote:\n",
        "          print( \"A\",A[self.num_of_hidden_lay-1].shape[0],A[self.num_of_hidden_lay-1].shape[1])\n",
        "          print( \"O\",O[self.num_of_hidden_lay-1].shape[0],O[self.num_of_hidden_lay-1].shape[1])\n",
        "\n",
        "\n",
        "       # print(O[-1])\n",
        "        if Y is not None:\n",
        "            loss = -(Y * np.log(O[self.num_of_hidden_lay]) + (1 - Y) * np.log(1.0 - O[self.num_of_hidden_lay]))\n",
        "            loss = loss.sum() / X.shape[0]\n",
        "        else:\n",
        "            loss = np.nan\n",
        "\n",
        "        if do_backward:\n",
        "            A_grad = np.zeros(self.deklarowanietablic, dtype=object)\n",
        "            O_grad = np.zeros(self.deklarowanietablic, dtype=object)\n",
        "\n",
        "            A_grad[self.num_of_hidden_lay]=O[self.num_of_hidden_lay] - Y\n",
        "\n",
        "            for i in range(self.num_of_hidden_lay,0,-1):\n",
        "              # print(i)\n",
        "              O_grad[i-1]=A_grad[i] @ self.hidden[i][0]\n",
        "              A_grad[i-1] = O_grad[i-1] * sigmoid(A[i-1],True)\n",
        "\n",
        "            for i in range(self.num_of_hidden_lay,1,-1):\n",
        "\n",
        "              self.hidden_grad[i][1]=A_grad[i].sum(0)/A_grad[i].shape[0]\n",
        "              self.hidden_grad[i][0]=A_grad[i].T @ O[i-1]/A_grad[i].shape[0]\n",
        "\n",
        "            self.hidden_grad[1][1]=A_grad[1].sum(0)/A_grad[1].shape[0]\n",
        "            self.hidden_grad[1][0]=A_grad[1].T @ X/ A_grad[1].shape[0]\n",
        "            #ciekwi mnie cemu nie dzialalo mi z [:]\n",
        "        #print(O[self.num_of_hidden_lay])\n",
        "        return O[self.num_of_hidden_lay], loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def gradient_descent_Nlayer(net, alpha, X, Y):\n",
        "    for i in range(100000):\n",
        "        net_result, loss = net.forward(X, Y, do_backward=True)\n",
        "        if (i % 5000) == 0:\n",
        "            print(f\"after {i} steps \\tloss={loss}\")\n",
        "        param = getattr(net, \"hidden\")\n",
        "        grad=getattr(net, \"hidden\" + \"_grad\")\n",
        "        for i in range(1,len(param)):\n",
        "          # print(\"param\",param[i][0] )\n",
        "\n",
        "          # print(\"param2\",alpha *grad[i][0])\n",
        "          # print(\"wyn\",param[i][0] - alpha *grad[i][0])\n",
        "          param[i][1] = param[i][1] - alpha *grad[i][1]\n",
        "          param[i][0] = param[i][0] - alpha *grad[i][0]\n",
        "\n",
        "          #print(\"paramwyn\",param[:])\n",
        "    return net_result, loss\n"
      ],
      "metadata": {
        "id": "7WRQq06qvXTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dims = [[4, 1], [4, 2],[4,3],[8,2]]\n",
        "for numofneurons ,numoflayers in dims:\n",
        "    print(f\"size = ({numoflayers}, {numofneurons})\")\n",
        "    net = NLayerNet(3,numoflayers,numofneurons  , dtype=np.float64)\n",
        "    res, loss = gradient_descent_Nlayer(net, alpha=0.1, X=X3, Y=Y3)\n",
        "    print(\"\\nLOSS:\", loss)\n",
        "    print(\"RESULT:\")\n",
        "    for input, output, y in zip(X3, res, Y3):\n",
        "        print(f\"XOR({input}) = {output}  Y: {y}\")\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "CuQLwsdZuMaz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "582be45c-5d45-4650-9682-6cc42bac129e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "size = (1, 4)\n",
            "after 0 steps \tloss=0.7363929638185444\n",
            "after 5000 steps \tloss=0.6925985716964397\n",
            "after 10000 steps \tloss=0.4835284359473316\n",
            "after 15000 steps \tloss=0.01898565211708713\n",
            "after 20000 steps \tloss=0.007247622971145021\n",
            "after 25000 steps \tloss=0.004321362447257685\n",
            "after 30000 steps \tloss=0.003040389803877297\n",
            "after 35000 steps \tloss=0.002331151311701569\n",
            "after 40000 steps \tloss=0.0018837287236576255\n",
            "after 45000 steps \tloss=0.0015769541341270823\n",
            "after 50000 steps \tloss=0.001354089834364707\n",
            "after 55000 steps \tloss=0.001185152052274915\n",
            "after 60000 steps \tloss=0.0010528522474600181\n",
            "after 65000 steps \tloss=0.0009465420851785327\n",
            "after 70000 steps \tloss=0.0008593147827158909\n",
            "after 75000 steps \tloss=0.0007864996575597492\n",
            "after 80000 steps \tloss=0.0007248278241408549\n",
            "after 85000 steps \tloss=0.0006719446684896456\n",
            "after 90000 steps \tloss=0.0006261121064423396\n",
            "after 95000 steps \tloss=0.0005860198250044655\n",
            "\n",
            "LOSS: 0.0005506683723498138\n",
            "RESULT:\n",
            "XOR([1. 1. 1.]) = [0.99967147]  Y: [1.]\n",
            "XOR([1. 1. 0.]) = [0.00057983]  Y: [0.]\n",
            "XOR([1. 0. 1.]) = [0.00068272]  Y: [0.]\n",
            "XOR([1. 0. 0.]) = [0.99945969]  Y: [1.]\n",
            "XOR([0. 1. 1.]) = [0.00046238]  Y: [0.]\n",
            "XOR([0. 1. 0.]) = [0.99928591]  Y: [1.]\n",
            "XOR([0. 0. 1.]) = [0.9993675]  Y: [1.]\n",
            "XOR([0. 0. 0.]) = [0.00046372]  Y: [0.]\n",
            "\n",
            "\n",
            "size = (2, 4)\n",
            "after 0 steps \tloss=0.6943523235103628\n",
            "after 5000 steps \tloss=0.6908220401582026\n",
            "after 10000 steps \tloss=0.2573978362991941\n",
            "after 15000 steps \tloss=0.00608195745696625\n",
            "after 20000 steps \tloss=0.002648231743633798\n",
            "after 25000 steps \tloss=0.0016528915566562238\n",
            "after 30000 steps \tloss=0.0011898611206773773\n",
            "after 35000 steps \tloss=0.0009247544154788164\n",
            "after 40000 steps \tloss=0.0007539028440541038\n",
            "after 45000 steps \tloss=0.0006350122470913054\n",
            "after 50000 steps \tloss=0.0005476991399979367\n",
            "after 55000 steps \tloss=0.0004809633574110824\n",
            "after 60000 steps \tloss=0.00042835987647231845\n",
            "after 65000 steps \tloss=0.00038586853921392464\n",
            "after 70000 steps \tloss=0.0003508548663359563\n",
            "after 75000 steps \tloss=0.0003215220993839815\n",
            "after 80000 steps \tloss=0.0002966037099126815\n",
            "after 85000 steps \tloss=0.00027518182060651687\n",
            "after 90000 steps \tloss=0.0002565753138395106\n",
            "after 95000 steps \tloss=0.00024026834547196738\n",
            "\n",
            "LOSS: 0.00022586594508584471\n",
            "RESULT:\n",
            "XOR([1. 1. 1.]) = [0.99989674]  Y: [1.]\n",
            "XOR([1. 1. 0.]) = [0.00024588]  Y: [0.]\n",
            "XOR([1. 0. 1.]) = [0.00023843]  Y: [0.]\n",
            "XOR([1. 0. 0.]) = [0.99971905]  Y: [1.]\n",
            "XOR([0. 1. 1.]) = [0.00023362]  Y: [0.]\n",
            "XOR([0. 1. 0.]) = [0.99972682]  Y: [1.]\n",
            "XOR([0. 0. 1.]) = [0.99971779]  Y: [1.]\n",
            "XOR([0. 0. 0.]) = [0.00014919]  Y: [0.]\n",
            "\n",
            "\n",
            "size = (3, 4)\n",
            "after 0 steps \tloss=0.7254000135764205\n",
            "after 5000 steps \tloss=0.6931486420424682\n",
            "after 10000 steps \tloss=0.6931484899544063\n",
            "after 15000 steps \tloss=0.693148351043076\n",
            "after 20000 steps \tloss=0.6931482237051714\n",
            "after 25000 steps \tloss=0.6931481065671667\n",
            "after 30000 steps \tloss=0.693147998445766\n",
            "after 35000 steps \tloss=0.69314789831608\n",
            "after 40000 steps \tloss=0.69314780528584\n",
            "after 45000 steps \tloss=0.6931477185743666\n",
            "after 50000 steps \tloss=0.6931476374953117\n",
            "after 55000 steps \tloss=0.693147561442405\n",
            "after 60000 steps \tloss=0.6931474898776231\n",
            "after 65000 steps \tloss=0.6931474223213087\n",
            "after 70000 steps \tloss=0.6931473583438776\n",
            "after 75000 steps \tloss=0.6931472975588204\n",
            "after 80000 steps \tloss=0.6931472396167662\n",
            "after 85000 steps \tloss=0.6931471842004215\n",
            "after 90000 steps \tloss=0.6931471310202322\n",
            "after 95000 steps \tloss=0.6931470798106447\n",
            "\n",
            "LOSS: 0.6931470303366061\n",
            "RESULT:\n",
            "XOR([1. 1. 1.]) = [0.49990781]  Y: [1.]\n",
            "XOR([1. 1. 0.]) = [0.49997842]  Y: [0.]\n",
            "XOR([1. 0. 1.]) = [0.49997048]  Y: [0.]\n",
            "XOR([1. 0. 0.]) = [0.50004227]  Y: [1.]\n",
            "XOR([0. 1. 1.]) = [0.49995577]  Y: [0.]\n",
            "XOR([0. 1. 0.]) = [0.50002527]  Y: [1.]\n",
            "XOR([0. 0. 1.]) = [0.50002316]  Y: [1.]\n",
            "XOR([0. 0. 0.]) = [0.50009321]  Y: [0.]\n",
            "\n",
            "\n",
            "size = (2, 8)\n",
            "after 0 steps \tloss=0.6983018760346127\n",
            "after 5000 steps \tloss=0.6929170442262746\n",
            "after 10000 steps \tloss=0.692349243378818\n",
            "after 15000 steps \tloss=0.6760463746014918\n",
            "after 20000 steps \tloss=0.006966300954330426\n",
            "after 25000 steps \tloss=0.002196063025881047\n",
            "after 30000 steps \tloss=0.001249284007791367\n",
            "after 35000 steps \tloss=0.0008599178428325648\n",
            "after 40000 steps \tloss=0.0006506653111788742\n",
            "after 45000 steps \tloss=0.0005209865813182983\n",
            "after 50000 steps \tloss=0.00043313704513710174\n",
            "after 55000 steps \tloss=0.00036987789336577655\n",
            "after 60000 steps \tloss=0.0003222531144868636\n",
            "after 65000 steps \tloss=0.00028516283792002296\n",
            "after 70000 steps \tloss=0.000255496019024832\n",
            "after 75000 steps \tloss=0.0002312501826829058\n",
            "after 80000 steps \tloss=0.0002110796713906371\n",
            "after 85000 steps \tloss=0.0001940476688382763\n",
            "after 90000 steps \tloss=0.00017948242709940612\n",
            "after 95000 steps \tloss=0.00016689003755776272\n",
            "\n",
            "LOSS: 0.0001559014983280273\n",
            "RESULT:\n",
            "XOR([1. 1. 1.]) = [0.99987965]  Y: [1.]\n",
            "XOR([1. 1. 0.]) = [0.0001213]  Y: [0.]\n",
            "XOR([1. 0. 1.]) = [0.00013742]  Y: [0.]\n",
            "XOR([1. 0. 0.]) = [0.99978872]  Y: [1.]\n",
            "XOR([0. 1. 1.]) = [0.00018361]  Y: [0.]\n",
            "XOR([0. 1. 0.]) = [0.99984279]  Y: [1.]\n",
            "XOR([0. 0. 1.]) = [0.9998547]  Y: [1.]\n",
            "XOR([0. 0. 0.]) = [0.00017065]  Y: [0.]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWuv7Q77-Nut"
      },
      "source": [
        "## Problem 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avuvSoWY-N4Z"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "deepnote": {},
    "deepnote_app_layout": "article",
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "1f038069-bae5-44e5-92ba-67bdff2c54a6",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}